{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning on Titanic passengers survival data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the Weaveworks MLOps profile version of `titanic_dataset_ml`. It retrieves datasets from S3 rather than local volumes.\n",
        "\n",
        "This Notebook will use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. The dataset provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age and survival.\n",
        "\n",
        "Credit for this Notebook goes to Niklas Donges, who published a very detailed post [here](https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8). Check it out if you want to dive deeper in the data analysis and machine learning details of the challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install dependencies as user packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "!pip3 install --user -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import dependencies and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "imports"
        ]
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from botocore.client import Config\n",
        "import io\n",
        "\n",
        "import tempfile\n",
        "import joblib\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import style\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:loaddata"
        ]
      },
      "outputs": [],
      "source": [
        "s3 = boto3.client('s3',\n",
        "    endpoint_url='http://minio-service.kubeflow:9000',\n",
        "    aws_access_key_id='minio',\n",
        "    aws_secret_access_key='minio123',\n",
        "    config=Config(signature_version='s3v4'),\n",
        "    region_name='')\n",
        "\n",
        "PREDICTION_LABEL = 'Survived'\n",
        "\n",
        "train_obj = s3.get_object(Bucket='mlops-profile-titanic', Key='train.csv')\n",
        "test_obj  = s3.get_object(Bucket='mlops-profile-titanic', Key='test.csv')\n",
        "\n",
        "train_df  = pd.read_csv(io.BytesIO(train_obj['Body'].read()))\n",
        "test_df   = pd.read_csv(io.BytesIO(test_obj['Body'].read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Let's explore the data\n",
        "\n",
        "These are features of the dataset:\n",
        "```\n",
        "survival:    Survival \n",
        "PassengerId: Unique Id of a passenger. \n",
        "pclass:    Ticket class     \n",
        "sex:    Sex     \n",
        "Age:    Age in years     \n",
        "sibsp:    # of siblings / spouses aboard the Titanic     \n",
        "parch:    # of parents / children aboard the Titanic     \n",
        "ticket:    Ticket number     \n",
        "fare:    Passenger fare     \n",
        "cabin:    Cabin number     \n",
        "embarked:    Port of Embarkation\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "train_df.head(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Missing data**\n",
        "\n",
        "Let's see here how much data is missing. We will have to fill the missing features later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "total = train_df.isnull().sum().sort_values(ascending=False)\n",
        "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
        "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
        "missing_data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Age and Sex**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "survived = 'survived'\n",
        "not_survived = 'not survived'\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
        "women = train_df[train_df['Sex']=='female']\n",
        "men = train_df[train_df['Sex']=='male']\n",
        "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
        "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
        "ax.legend()\n",
        "ax.set_title('Female')\n",
        "ax.set_ylabel('Survival Probablity')\n",
        "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
        "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n",
        "ax.legend()\n",
        "ax.set_title('Male')\n",
        "_ = ax.set_ylabel('Survival Probablity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Embarked, Pclass and Sex**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "FacetGrid = sns.FacetGrid(train_df, row='Embarked', height=4.5, aspect=1.6)\n",
        "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\n",
        "_ = FacetGrid.add_legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pclass**\n",
        "\n",
        "Explore if `Pclass` is contributing to a person chance of survival "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "_ = sns.barplot(x='Pclass', y='Survived', data=train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we confirm that being in class 1 increases the chances of survival, and that a person in class 3 has high chances of not surviving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\n",
        "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
        "grid.add_legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATA PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SibSp and Parch\n",
        "\n",
        "Combine these two features as the number of relatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:datapreprocessing",
          "prev:loaddata"
        ]
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n",
        "    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n",
        "    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n",
        "    dataset['not_alone'] = dataset['not_alone'].astype(int)\n",
        "train_df['not_alone'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "skip"
        ]
      },
      "outputs": [],
      "source": [
        "# Survival with respect to the number of relatives in the ship\n",
        "axes = sns.catplot('relatives','Survived', kind='point',\n",
        "                      data=train_df, aspect = 2.5, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This does not contribute to a person survival probability\n",
        "train_df = train_df.drop(['PassengerId'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Missing data: Cabin\n",
        "\n",
        "Create a new `Deck` feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
        "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
        "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
        "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
        "    dataset['Deck'] = dataset['Deck'].astype(int)\n",
        "# we can now drop the cabin feature\n",
        "train_df = train_df.drop(['Cabin'], axis=1)\n",
        "test_df = test_df.drop(['Cabin'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Missing data: Age\n",
        "\n",
        "Fill missing data from age feature with a random sampling from the distribution of the existing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    mean = train_df[\"Age\"].mean()\n",
        "    std = test_df[\"Age\"].std()\n",
        "    is_null = dataset[\"Age\"].isnull().sum()\n",
        "    # compute random numbers between the mean, std and is_null\n",
        "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
        "    # fill NaN values in Age column with random values generated\n",
        "    age_slice = dataset[\"Age\"].copy()\n",
        "    age_slice[np.isnan(age_slice)] = rand_age\n",
        "    dataset[\"Age\"] = age_slice\n",
        "    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n",
        "train_df[\"Age\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Missing data: Embarked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df['Embarked'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fill with most common value\n",
        "common_value = 'S'\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convert Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:featureengineering",
          "prev:datapreprocessing"
        ]
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Fare'] = dataset['Fare'].fillna(0)\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Titles features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "\n",
        "for dataset in data:\n",
        "    # extract titles\n",
        "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    # replace titles with a more common title or as Rare\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
        "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    # convert titles into numbers\n",
        "    dataset['Title'] = dataset['Title'].map(titles)\n",
        "    # filling NaN with 0, to get safe\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "train_df = train_df.drop(['Name'], axis=1)\n",
        "test_df = test_df.drop(['Name'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sex into numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "genders = {\"male\": 0, \"female\": 1}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Sex'] = dataset['Sex'].map(genders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Drop Ticket feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = train_df.drop(['Ticket'], axis=1)\n",
        "test_df = test_df.drop(['Ticket'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embarked into numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].map(ports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Age into categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n",
        "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n",
        "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n",
        "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n",
        "    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n",
        "\n",
        "# let's see how it's distributed train_df['Age'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fare into categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n",
        "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n",
        "    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Age times Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age_Class']= dataset['Age']* dataset['Pclass']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fare per person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dataset in data:\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n",
        "# Let's take a last look at the training set, before we start training the models.\n",
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML\n",
        "\n",
        "Because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels = train_df[PREDICTION_LABEL]\n",
        "train_df = train_df.drop(PREDICTION_LABEL, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:randomforest",
          "prev:featureengineering"
        ]
      },
      "outputs": [],
      "source": [
        "random_forest = RandomForestClassifier(n_estimators=100)\n",
        "random_forest.fit(train_df, train_labels)\n",
        "acc_random_forest = round(random_forest.score(train_df, train_labels) * 100, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:logisticregression",
          "prev:featureengineering"
        ]
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(solver='lbfgs')\n",
        "logreg.fit(train_df, train_labels)\n",
        "acc_log = round(logreg.score(train_df, train_labels) * 100, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:naivebayes",
          "prev:featureengineering"
        ]
      },
      "outputs": [],
      "source": [
        "gaussian = GaussianNB()\n",
        "gaussian.fit(train_df, train_labels)\n",
        "acc_gaussian = round(gaussian.score(train_df, train_labels) * 100, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:svm",
          "prev:featureengineering"
        ]
      },
      "outputs": [],
      "source": [
        "linear_svc = LinearSVC()\n",
        "linear_svc.fit(train_df, train_labels)\n",
        "acc_linear_svc = round(linear_svc.score(train_df, train_labels) * 100, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:decisiontree",
          "prev:featureengineering"
        ]
      },
      "outputs": [],
      "source": [
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(train_df, train_labels)\n",
        "acc_decision_tree = round(decision_tree.score(train_df, train_labels) * 100, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:results",
          "prev:randomforest",
          "prev:logisticregression",
          "prev:naivebayes",
          "prev:svm",
          "prev:decisiontree"
        ]
      },
      "outputs": [],
      "source": [
        "model_names  = ['Support Vector Machines', 'logistic Regression', 'Random Forest', 'Naive Bayes', 'Decision Tree']\n",
        "model_scores = [acc_linear_svc, acc_log, acc_random_forest, acc_gaussian, acc_decision_tree]\n",
        "models = [linear_svc, logreg, random_forest, gaussian, decision_tree]\n",
        "results = pd.DataFrame({\n",
        "    'Model': model_names,\n",
        "    'Score': model_scores})\n",
        "result_df = results.sort_values(by='Score', ascending=False)\n",
        "result_df = result_df.set_index('Score')\n",
        "print(result_df)\n",
        "\n",
        "best_name  = result_df['Model'].iloc[0]\n",
        "best_index = model_names.index(best_name)\n",
        "best_model = models[best_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "block:storebestmodel",
          "prev:results"
        ]
      },
      "outputs": [],
      "source": [
        "s3 = boto3.client('s3',\n",
        "    endpoint_url='http://minio-service.kubeflow:9000',\n",
        "    aws_access_key_id='minio',\n",
        "    aws_secret_access_key='minio123',\n",
        "    config=Config(signature_version='s3v4'),\n",
        "    region_name='')\n",
        "\n",
        "with tempfile.TemporaryFile() as fp:\n",
        "    joblib.dump(best_model, fp)\n",
        "    fp.seek(0)\n",
        "    s3.put_object(Bucket='mlops-profile-titanic', Key='model.joblib', Body=fp.read())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kubeflow_notebook": {
      "experiment_name": "titanic_s3",
      "pipeline_description": "Titanic S3",
      "pipeline_name": "ml-s3-1",
      "volumes": []
    },
    "kubeflow_noteobok": {
      "docker_image": "chanwit/titanic-mlops-base:619b15dc",
      "experiment_name": "Titanic S3",
      "pipeline_description": "Titanic S3",
      "pipeline_name": "ml-comparison-s3",
      "volumes": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
